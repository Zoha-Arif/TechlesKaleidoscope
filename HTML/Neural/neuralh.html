<!DOCTYPE html>
<html>
<head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <title>Neural +_+</title>
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="stylesheet" type="text/css" media="screen" href="../../CSS/main.css"/>
    <script src="main.js"></script>
    <script src="../../JavaScript/jquery-3.3.1.min.js" type="text/javascript"></script>
    <script src="../../JavaScript/main.js" type="text/javascript"></script>
    <script src="../../JavaScript/main2.js" type="text/javascript"></script>
    <link href="https://fonts.googleapis.com/css?family=Source+Code+Pro" rel="stylesheet">  
    <link href="https://fonts.googleapis.com/css?family=Mukta:400,500" rel="stylesheet"> 
</head>
<body id="white_background">
    <div>
        <nav>
            <a id="k1" class="navc" class="navb" href="../index.html">Home</a>
            <a id="k2" class="navc" class="navb" href="../Kaleidoscope.html">The Kaleidoscope</a>
            <a id="k3" class="navc" class="navb" href="../Contact.html">Contact</a>
            <a id="k4" class="navc" class="navb" href="../About.html">About</a>
        </nav>
    </div>
    <div id="lesson">
        <center><h2 class="codefont" class="title1">Introduction to Deep Learning & Neural Networks: Create, from Scratch, Neural Networks in Java and Python Using Keras, Tensorflow, and DL4J</h2></center>
    </div>
    <p class="lesson">
            In a science fiction Star Wars universe, machines and automated robots have the strange ability to recognize faces and vehicles, understand language, and predict viable solutions. 
             A while ago, I found myself wondering if it is possible to create the stereotypical intelligent sci-fi robot, and so I began to research computer vision -- this awesome branch 
             of programming that gives a computer human intelligence and instincts to see, identify and process images in the same way that human vision does, and then provide an appropriate
              output. After days of delving into computer vision research, I accidently stumbled upon neural networks, this extremely complicated function that allotts machines the 
              capability to “see”. Enticed, I rummaged through about a 100 computer vision courses, youtube series, and research papers, to understand how neural networks enable machines to 
              see, only to finally understand that the concept of machines learning human intelligence was not at all the imaginative futuristic impossible program that I had always believed.
               In reality, the programs that make RD2 and C3PO intelligent really just come down to some calculus and intermediate programming skills. 
        <br>
        <br>
            But that’s not to say neural networks still aren’t awesome. I think that, regardless of the method, there will always be something magical and not so straight-forwardly
             mathematic about machines having human capabilities. So for those of you who are interested in computer vision, welcome to the first installment of this very exciting neural
              network series. There are plenty of neural network courses from youtubers such as Sentdex and Michael Nielson on building neural network datasets and this course will build 
              upon those courses by diving into the monstrous deed of creating a completely original neural network from scratch. Over the past 5 months, I have thrown my weekends away to
               developing, from scratch, one original neural network: a model that recognizes American Sign Language hand gestures and outputs the interpretation of those hand gestures. To 
               further understand how neural networks work and how to construct them, I have supplemented my experimentation with tutorials, among which the major contributors were youtuber 
               Sentdex, Andrew Ng’ Convolutional Neural Network’s course, and Michael Neilson’s Introduction to Neural Networks, and all learners are highly encouraged to check out these 
               courses. 
            
    </p>
    <h3 class="codefont" id="justify1">Lesson One: An Innocent Introduction to Neural Networks</h3>
    <p class="lesson">
            Humankind can effortlessly (and very deceptively) recognize digits like 5, 0, 9, and 2, regardless of handwriting style and size because we have a type of supercomputer in our brains. This supercomputer contains an entire series of visual cortices each containing over 100 million neurons with tens of billion connections between them. These neurons complete, in seconds, complex image processing inputted from the visual world, allowing us to effortlessly recognize digits, letters, words, and objects. But recognizing something as simple as the number one isn’t as easy as we humans make it seem because all of the complex work is done unconsciously for us. The difficulty of visual pattern recognition becomes apparent if you attempt to write a computer program to do so. We tend to recognize numbers and other objects based on characteristics. For example, we know that “8” has a loop at the top and bottom without any vertical strokes while 1’s are usually one long vertical stroke. These characteristics, however, are not simple to express algorithmically and exceptions makes it relatively impossible and complicated to write an efficient algorithm. After all, unlike a person, you can’t simply command a computer program to memorize the characteristics of different objects and digits. This derived problem is where neural networks become essential. As the name suggests, neural networks are inspired by the way our human visual cortex system works. Officially, artificial neural networks simulate a network of neurons so that the computer “learns” to visually recognize objects as the human brain does. (Don’t worry if that doesn’t make too much sense now!)
        <br>
        <br>
            And in the rare circumstance that you haven’t heard, neural networks are hot because of their organic learning style and their non-linear data processing, above other things. 
            <br>
            <br>    
            <b>Insert comic of hot neural network.</b> 
        <br>
        <br>
            In the past 10 years, the best-performing artificial-intelligence systems from Google’s automatic translator to speech translation algorithms have their roots with neural networks and the truth of the matter is that the field of deep learning has been rapidly industrializing since 1943 when neurophysiologist Warren McCulloch and mathematician Walter Pitts modeled the world’s first simple neural network using electrical circuits. So, to say the least, neural networks are fundamentally and algorithmic extremely complicated but applicable. 
        <br>
        <br>    
            The classic “Hello World!” example of image processing neural networks is a model that learns to recognize handwritten digits. We will not be building this neural network because there are plenty of other written and video tutorials on the MNIST dataset. However, this neural network is a great first neural network because it’s an excellent challenging yet not too complicated prototype that can aid in developing the advanced techniques needed for more complicated networks and other problems such as natural language processing. Before we even think about building any neural network, however, we have quite a few concepts to understand.
        <br>
        <br>
            Fundamentally, how do neural networks work? 
        <br>
        <br>
            In the most simple manner, neural networks take a large number of images depicting the object to be recognized, such as handwritten digits, known as training examples. The more training examples, the more data and learning material the networks has to learn, and so the more training examples, the better the accuracy. So while 100 training examples may be sufficient, industry and professional neural networks usually have thousands, millions, and even billions of training examples. Through mathematics, the network is “trained” to recognize specific handwritten digits based off those training images. The network then develops a system, or set of rules, which can learn patterns from those training examples. You can think of the workflow of neural networks this way: when a toddler learns how a number eight or a number seven looks, the instructor shows that child examples of number eights or sevens so that the toddler can learn that a number eight has too loops stacked on top of each other and that a number seven has a horizontal line that turns sharply down at a vertical slope. Similarly, for a neural network to learn how to recognize handwritten digits, examples, training examples, thousands, millions, and billions of them have to be fed into the network. 
        <br>
        <br>
            But this explanation is barely scratching the surface of neural networks. How exactly should the dataset be structured and how is the dataset inputted to the network? How is the network “trained” and what is happening when the network is “learning”? Questions like this are essential to the understanding of deep learning. 
            
    </p>

<style>
    #white_background {
        background-color: white; 
    }

    #lesson {
        text-align: justify;
        margin-left: 230px; 
        margin-right: 230px;  
    }

    .lesson {
        text-align: justify;
        margin-left: 230px; 
        margin-right: 230px;
        font-family: 'Mukta', sans-serif;
        line-height: 20px; 
    }

    #justify1 {
        text-align: justify;
        margin-left: 230px; 
        margin-right: 230px;
    }

    .navc {
    background-color: #241F1F; 
    border: none;
    color: white;
    padding: 15px 32px;
    text-align: center;
    text-decoration: none;
    display: inline-block;
    font-size: 16px;
    font-family: 'Source Code Pro', monospace;
    position: relative; 
    left: 50%;
    }

    nav {
        background-color: #241F1F; 
    }

    #title1 {

    }
</style>
</body>